---
title: Tracking performance over the entire software lifecyle
author: Nicolas Mattia
---

Here's a standard workflow:

1. Your manager tells you clients have been complaining about the
   performance of frobnicating aadvarks. Your manager wants you to
   solve that.
1. You quickly put together a few benchmarks that characterize the
   cost of frobnication.
1. You then spend the next few days or weeks playing _the game_: look
   at benchmark numbers, make a change, rerun benchmarks, compare,
   commit if the numbers look better. Rinse and repeat.

What's missing is the notion of _time_. How do you know that as you
work on the product for the _next few months_, you aren't introducing
any performance regressions? Better still, how do you visualize what
progress has been made over the _last few months_ (or years!), when
clients were already complaining about performance and you had already
made some improvements?

Tracking performance over time is hard. For one, results need to be
stored in a shared data store, that can be reused across many
benchmark runs. Further, there needs to be an easy way to visualize
these results as trend lines and to perform further analyses on an *ad
hoc* basis. I'll show you how to do that with Elasticsearch for safe
and accessible storage, Kibana for visualization and
a [Criterion-like][hyperion] framework for writing benchmarks.

## Benchmarking is hard

Sitting somewhere between coffee and unit tests on the list of many
programmers' acquired tastes, benchmarks are an important tool whenever you
need to monitor or improve a system's performance. Before even starting to
think about _how_ you are going to optimize a particular program, it is
important that you know _why_ (and _if_) you need to gain performance, and
_where_ there is performance to be gained.

>  We should forget about small efficiencies, say about 97% of the time:
>  premature optimization is the root of all evil.
>   - Donald Knuth

Measuring software performance through micro-benchmarks is pretty much a solved
problem --- at least in the Haskell space --- thanks to the amazing
[criterion][criterion] library. However there is still an open question about
how to present the benchmark results to the programmer. [criterion][], for
instance, presents some analysis (like mean duration and standard deviation).

However isolated benchmarks are rarely useful. You rarely care about exactly
how long it takes for a function to run. You want to be able to compare
benchmark results: before this commit, over time, etc. Which leads to other
problems like ensuring that all the benchmarks are run on the same hardware and
how to store those benchmarks. Some solutions have emerged for particular use
cases (see [rust's solution](https://perf.rust-lang.org/), [GHC's
solution](https://perf.haskell.org/ghc/) and [ETA's
solution](https://www.reddit.com/r/haskell/comments/5nczqt/eta_modern_haskell_on_the_jvm/))
but it's impractical to roll out a new solution for every single project.

The tools mentioned above allow the benchmarks to be compared and visualized.
But there are are off-the-shelf tools that allow just that: Elastic Search,
which allows you to store, search and analyze your data, and Kibana, an Elastic
Search plugin, that allows you to visualize just about anything.

We'll use [`hyperion`][hyperion] to perform the benchmarks and generate the
data. We'll upload the data to Elastic Search. We rely on Kibana to visualize
the data and get insight.

Let's see what this looks like in practice.

### Setting up Elastic Search

We'll setup local instances of Elastic Search and Kibana. The easiest way is to
spin up ready-made containers, for instance using the
[docker-elk](https://github.com/deviantony/docker-elk/tree/f7f08449f2ce6b5f4f78e707af5b61c8ffcc7e91)
project. It uses [docker-compose](...) to start some containers: Elastic Search
on port 9200, Kibana on port 5600 (it also starts a Logstash container but we
won't be using it). There we go:

``` shell
$ git clone git@github.com:deviantony/docker-elk.git
$ cd docker-elk
$ docker-compose up -d
$ curl localhost:9200
{
  "name" : "7pOsdaA",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "e_CQlDgCQ1qIHob-n9h3fA",
  "version" : {
    "number" : "5.5.2",
    "build_hash" : "b2f0c09",
    "build_date" : "2017-08-14T12:33:14.154Z",
    "build_snapshot" : false,
    "lucene_version" : "6.6.0"
  },
  "tagline" : "You Know, for Search"
}
```

We now need to configure Elastic Search. Time to get your JSON goggles, because
we'll use JSON as the lingua franca, and there's going to be plenty of it from
now on.



We'll create an ES index. The index is similar to a database. We will create a
JSON index specification. The index requires a "type" (here `all-benchmarks`),
which is similar to a table (we won't go into the details but go have a look at
the
[documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html#create-index-settings)
if you're interested):

``` json
{
  "settings" : {
      "number_of_shards" : 1
  },
  "mappings" : {
    "all-benchmarks" : {
      "dynamic" : true,
      "properties" : {
        "bench_name": {
          "type": "keyword"
        },
        "time_in_nanos": {
          "type": "float"
        },
        "timestamp": {
          "type": "date",
          "format": "epoch_second"
        }
      }
    }
  }
}
```

When using Elastic Search it is much easier to work with flat JSON documents
without nested fields. Here's a JSON document that fits with the index above,
and that we find works well for benchmark data:

``` json
{
    "bench_name": "fibonacci",
    "timestamp": "1504081132",
    "time_in_nanos": 240
}
```

Let's break down the index fields. `properties` specifies which fields we want
to be present when a new ES document is uploaded. In this case we'll have

 * `bench_name`: the benchmark name, as a `keyword`, which is a string field
   type that ES can index
 * `time_in_nanos`: the measured execution time for the given benchmark
 * `timestamp`: an ES [date](todo.com) which will be used as the canonical
   timestamp (to clarify we specify the format, the number of seconds since the
   epoch).

`dynamic: true`: this tells ES that other fields might be present and that it
should try to index them as well. Let's save the index definition to a file
`index.json` and upload it as index `hyperion-test`:

``` shell
$ curl -X PUT 'http://localhost:9200/hyperion-test' --data @index.json
```

Elastic Search is set up!

### Generating the data

In order to upload the benchmark data you will need it in JSON format. How you
generate the data depends greatly on what kinds of systems you are going to
benchmark and what programming language you are using. At Tweag I/O, when
applicable, we like to use Haskell. The ecosystem already contains some great
benchmarking tools like [criterion][criterion]. However criterion doesn't allow
to manipulate the data the way we need it to, so we wrote [hyperion][hyperion].

While [criterion][criterion] is great for measuring performance and analyzing
results, [`hyperion`][hyperion] takes a different approach: it provides enough
information for dedicated tools to interact with the data. Such tools could
perform regressions, plots, or anything else you could dream of. Hyperion
benchmarks your code and passes the collected data further.

[hyperion][hyperion] is not a replacement for [criterion][criterion], rather it
is a place for trying out new ideas. We wanted a lab where we could develop and
experiment with new features, which might in turn be contributed to
[criterion][criterion] itself. Hyperion includes features like recording the
benchmark input in the report and manipulating the output JSON document, which
we'll use in a minute.

We'll use hyperion's [micro-benchmark
example](https://github.com/tweag/hyperion/blob/91df3b9edc0945a1660e875e37f494e54b1419f5/examples/micro-benchmarks.hs)
to generate the data:

``` haskell
benchmarks :: [Benchmark]
benchmarks =
    [ bench "id" (nf id ())
    , series [0,5..20] $ \n ->
        bgroup "pure-functions"
          [ bench "fact" (nf fact n)
          , bench "fib" (nf fib n)
          ]
    , series [1..4] $ \n ->
        series [1..n] $ \k ->
          bench "n choose k" $ nf (uncurry choose) (n, k)
    ]

main :: IO ()
main = defaultMain "hyperion-example-micro-benchmarks" benchmarks
```

If you've used [criterion][criterion] before the functions `bench` and
`benchGroup` will look familiar. `series` is a [hyperion][hyperion] function
that will run the benchmarks with different input while allowing the input to
be recorded in the report. Let's see what this looks like:

``` shell
$ stack exec hyperion-micro-benchmark-example -- --json -
```

The `--json -` argument tells hyperion to write a JSON report to stdout.
Hyperion generates a `metadata` section with general information, and a
`results` section containing the individual benchmark results. You'll see that
some benchmarks results contain a field `bench_params`: this is the input data
given by `series`.

``` json
{
  "metadata": {
    "location": null,
    "timestamp": "2017-08-30T08:36:14.282423Z"
  },
  "results": [
    ...
    {
      "alloc": null,
      "bench_name": "pure-functions:15/fib",
      "bench_params": [
        15
      ],
      "cycles": null,
      "garbage_collections": null,
      "measurements": null,
      "time_in_nanos": 4871.05956043956
    },
    ...
    ]
}
```

This format however is not ideal for working with Elastic Search for several
reasons:

* All the benchmark results are bundled within the same JSON object which means
  that they won't be indexed independently by ElasticSearch.
* The metadata section is barely useful: the `location` field does not provide
  any useful information and the `timestamp` field was sourced from system date
  which means it depends on when the benchmark was _run_, no when the code was
  committed.

We'll annotate the metadata with information gathered from `git` and we'll make
the format ES-friendly:

``` shell
$ alias git_unix_timestamp="git show -s --format=%ct HEAD"
$ alias git_commit_hash="git rev-parse HEAD"
$ stack exec hyperion-micro-benchmark-example -- \
    --arg timestamp:"$(git_unix_timestamp)" \
    --arg commit:"$(git_commit_hash)" \
    --flat -
```

We created two helpers for getting a unix timestamp and the latest commit's
hash from git and used that to annotate the metadata with hyperion's `--arg
key:val`. Then with `--flat -` we tell hyperion to write a flat version of the
JSON benchmark report to stdout, which is effectively a JSON array where each
element contains the information about a particular benchmark as well as a copy
of the metadata:

``` json
[
  ...
  {
    "alloc": null,
    "bench_name": "pure-functions:15/fib",
    "commit": "91df3b9edc0945a1660e875e37f494e54b1419f5",
    "cycles": null,
    "garbage_collections": null,
    "location": null,
    "measurements": null,
    "time_in_nanos": 4998.988241758242,
    "timestamp": "1504081805",
    "x_1": 15
  },
  ...
```

Very good, let's reuse that command and (with a little help of
[`jq`](https://stedolan.github.io/jq/manual/) and
[`xargs`](https://www.gnu.org/software/findutils/)) feed each element to
Elastic Search (in practice you'll want to use Elastic Search's [bulk
API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html)
but let's keep it simple for now):

``` shell
$ stack exec hyperion-micro-benchmark-example -- \
    --arg timestamp:"$(git_unix_timestamp)" \
    --arg commit:"$(git_commit_hash)" \
    --flat - \
    | jq -cM '.[]' \
    | xargs -I '{}' -d '\n' \
    curl -X POST 'http://localhost:9200/hyperion-test/all-benchmarks' \
        -H 'Content-Type: application/json' \
        -d '{}'
```

### Kibana's Way


As mentioned earlier, we also span up a Kibana container, accessible at
`http://localhost:5601`. However if try to access the page now you'll most
likely get an error, since we haven't configured Kibana yet. We need to tell it
what ES indices it should gather the data from. Let's tell Kibana to use any
index that's prefixed with `hyperion`. We'll first create an [index
pattern](https://www.elastic.co/guide/en/kibana/current/index-patterns.html)
index then tell Kibana to use that index pattern as the default:

``` shell
$ curl -X PUT 'http://localhost:9200/.kibana/index-pattern/hyperion*' \
    -H 'Content-Type: application/json' \
    -d '{"title": "all hyperion", "timeFieldName": "timestamp"}'
$ curl -X PUT 'http://localhost:9200/.kibana/config/5.5.2' \
    -H 'Content-Type: application/json' -d '{"defaultIndex": "all hyperion"}'
```

You should now be able to access Kibana by opening your browser at the address
[http://localhost:5601]. In the Discover landing page you should see a list of
the benchmark data that we uploaded to Elastic Search earlier. We'll create our
first visualization using that data by clicking the Visualize tab on the left
menu, and then "Create a new visualization", "Line". Then click the index
pattern that we created earlier ("hyperion*"). This should take you to a
dashboard where you can describe the visualization. This is where the magic
happens.

First let's make sure that our benchmark data is in scope by clicking the time
picker at the top right corner and set the time range to "Last 6 months"
(anything will work as long as the timestamp of the specific commit you tagged
your data with falls into that range). We can fill the panel on the left hand
side:

![Data settings](../img/posts/hyperion-data-settings.png "Data settings for visualization")

filter: `commit:"91df3b9edc0945a1660e875e37f494e54b1419f5" AND bench_name:pure-functions*fib`

Let's break this down:

* X-Axis Aggregation: By using Elastic Search's [term
  aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/5.5/search-aggregations-bucket-terms-aggregation.html)
  we're able to use one of the document's field as values for the X axis. Here
  we use `x_1` which is the parameter generated by `series` and used as an
  argument to `fib`. We set the Order to "Ascending" and Order By to "Term". If
  you nested several `series` you could pick `x_2`, `x_3`, ... as the X axis,
  allowing you to easily compare the effect of various parameters. Or you could
  use the `timestamp` parameter to see the evolution of your system's
  performance over time. Let's stick to `x_1` for now.
  TODO: update picture

* Y-Axis Aggregation: here we picked "Max" as the aggregation value. What this
  means is that when several values are available for a specific X value Kibana
  will only display the maximum value. If you've set the filter to use only the
  values of `pure-functions*fib` you should only get a single value anyway, but
  this can come in handy if you have more than one value and you want to
  perform custom regressions (see hyperion's `--raw` argument).

You should get a plot similar to the following image:

![Fibonacci vs input](../img/posts/hyperion-fib-vs-x1.png "Fibonacci plot vs input")

[hyperion]: https://github.com/tweag/hyperion
[criterion]: https://github.com/bos/criterion
