---
title: "Funflow"
shortTitle: "Funflow"
author: "Nicholas Clarke"
---

In this blog post, we'd like to officially announce Funflow, a system we've been
working on for the last few months with a small number of clients. Funflow is a
system for building and running workflows. In this blog post we'll talk about
what that entails, why we built funflow, and what we'd like to do with it in the future.

## What is a workflow?

At its core, a workflow takes some inputs and produce some outputs, possibly
producing some side effects along the way. Of course, this description basically
describes any program. Workflow systems distinguish themselves in a few ways:

- A workflow is often composed of a number of steps, which may themselves be
  independent programs (or workflows). Composition of this higher level of
  programs might be done by domain specialists rather than programmers.
- Workflows are often long running, with some likelihood of failure halfway
  through. In such cases, we would want to resume them midway through without
  needing to rerun the earlier stages.
- Likewise, workflows may often need to be run on clusters or across distributed
  systems.

Workflow systems have perhaps seen the most use in the business process space,
where they allow non-programmers to automate business processes (such as
authorising a purchase order) which may involve multiple steps, some of which
are automatic, and others which might involve human intervention.

Another area heavily involving workflows, and one in which we are particularly
interested, is in scientific data processing. For example, in bioinformatics
one might develop a workflow (also often called 'pipeline' in this space) to
perform some in-silico analysis of sequenced gene data. Such a workflow will
be customised for a particular task, and may then be run for each sequenced
sample.

## Wherefore funflow?

There are innumerable workflow systems already out there, with myriad different
features. Tools like [Apache Taverna](https://taverna.incubator.apache.org/)
provide a very mature solution for building enterprise workflows, with
integrations for working with systems such as Hadoop and powerful GUI editors
for composing workflows. At the other end of the scale,
[luigi](http://luigi.readthedocs.io) and [airflow](https://airflow.apache.org/)
are libraries for composing workflows in Python code.

So why build a new one? It mostly came down to two things: integration and
caching. We'll spend the majority of this post discussing caching, so let's talk
about integration first.

Most of the simple workflow systems we looked at were designed to be run from a
command line, by a person or at best by a tool like `cron` or `at`. We find,
however, that we often want to run a workflow as part of a larger application.
For example, when doing data analysis one might wish to run classifiers on
different subsets of data, and then visualise the results. We still want to be
able to use classifiers written in any language or running on a different
machine, but we should be able to track the progress in a surrounding program,
and easily get the results back. When we tried to do this with Luigi, we found
ourselves needing to parse the log output just to find out the ID of the job
we'd triggered! So integration, for us, means:

- First class support for pulling values into, and out of, the host program.
- Ability to trigger a pipeline from within your program and deal with its
  output as any other value.

Funflow's workflows are just Haskell programs, built using
[arrow](https://www.haskell.org/arrows/) syntax. With funflow, we can easily
intermix steps done _inside_ our Haskell process and steps done _outside_, for
example by another program. Arrows give us the generality to model multiple
types of computation - a function `a -> b` is an arrow, as is a monadic function
`Monad m => a -> m b`, or a stream transformer `Stream a -> Stream b`. But
because they're less powerful than monads, we also get the ability to introspect
them and do things like drawing the dependency graph.

### Caching

> There are two hard problems in programming: naming things, cache invalidation,
> and off by one errors. 

Consider the following two workflows, expressed as Haskell functions:

```haskell
flow1 :: Foo -> Bar
flow1 = extractBar . someExpensiveComputation . someTransformation

flow2 :: Foo -> Baz
flow2 = extractBaz . someExpensiveComputation . someTransformation
```

What we notice about these is that the first part of the computation is shared
between `flow1` and `flow2`. If we feed the same input `Foo`, we would
particularly like not to repeat `someExpensiveComputation`.

This is not an idle example; we often see workflows where part of the workflow
involves preprocessing of a reference data set, which may be done multiple
times, either by different users or when running a pipeline multiple times.
Perhaps more importantly, it may often be desirable to tweak the parameters of
some late stage of processing and rerun the pipeline - again, without rerunning
the unchanged earlier parts.

In order to address this issue, funflow borrows a couple of ideas from the
[nix](https://nixos.org/nix/) package manager. The first of these is to remove
the notion that the user has any control over where and how the outputs of the
intermediate steps in workflows are stored. Instead of the user controlling
where files are output, funflow manages a section of the file system known as
the _store_. Entries inside the store are addressed by a unique hash (the second
idea borrowed from nix), determined by hashing both the inputs to a step and the
definition of that step itself. When funflow executes a step in a workflow, it
first determines the hash of the inputs and the step definition to determine the
output path. If this path already exists (since store items are immutable once
written), we can skip the computation and use the result from the cache.

Funflow goes further than nix does, however. Whilst the hash of the inputs and
step definition determines the path to which the step writes its output, upon
completion of the step these outputs are moved into another path determined by
their _own_ hash: in other words, the store also works as [content addressable
storage](https://en.wikipedia.org/wiki/Content-addressable_storage). What's the
benefit of this? Well, firstly, it ensures that when multiple steps produce the
same output, that output is cached only once on disk. However, it also solves
the problem suggested by the following flows:

```haskell

flow1 :: Int -> Bar
flow1 = extractBar . someExpensiveComputation . (* 2)

flow2 :: String -> Bar
flow2 = extractBar . someExpensiveComputation . length
```

In this example, the _tails_ of the flows are similar. If I provide `4` to the
first flow and `"workflow"` to the second, however, then these computations will
converge after their first steps, and before `someExpensiveComputation`. If a
nix derivation were used in this case, two outputs will be produced and
`someExpensiveComputation` run twice, because ultimately the inputs differ.
Funflow, on the other hand, allows computations to _converge_.

Doing sensible caching might seem like a poor reason to build our own workflow
system. In reality, though, it's a large part of what makes one up. Going back
to our concepts above on what characterises a workflow as different from any
other program, funflow's system for managing inputs and outputs addresses them
pretty directly:

- Since long running pipelines are likely to fail, it's important to make sure
  we can resume them from the point of failure, even if we've modified the
  pipeline in order to address that failure. At the same time, we need to be
  sure that we don't accidentally find ourselves reusing cached data if it's
  inappropriate.
- In order to ship computations between machines, we need to be sure that the
  full environment to run that computation is available on the target machine.
  By making this environment explicit, and constraining what can be in it,
  funflow can make this safe and easy to do.

What's more, the same approach we take in doing caching correctly provides
solutions to a number of other important problems that arise in workflow
management. For example, one important component of scientific research is for
other teams to be able to reproduce the results underlying a paper. Funflow
makes it possible to capture the _closure_ for a given output, which can then be
distributed easily to other locations. By using docker containers for the
processing of individual steps, and by strictly controlling their inputs, we can
also ensure that computation is isolated from other parts of the system,
reducing the "noise" which can sometimes change results.

### Why not use nix?

We've written in the past about how we use nix at tweag, and mentioned above how
various features of funflow were inspired by it. So one might be tempted to
ask - why not just use nix?

One certainly could build such a workflow using nix, and this wouldn't be a bad
option. But there are a few reasons not to. One we mentioned above - nix doesn't
use caching as much as we'd like. Its model fits reasonably well with a package
manager, where a central build server can re-evaluate the entire tree when
low-level packages change, but not well with a workflow system where multiple
permutations of initial arguments might be needed.

For a second point, the major advantages of nix come from using the entirety of
the nix ecosystem. And where possible, we would definitely recommend this
ecosystem! However, often one might not have the desire or liberty to switch
over to a completely different package management system just to run an
analysis. The original client with whom we developed funflow has an existing
infrastructure upon which their system is based. Replacing or duplicating that
with nix was not an option for them.

Of course, one can use nix without the entire nix ecosystem - we have done so
previously in order to construct RPMs. But it feels like a hack to do so, and
loses many advantages.

The main reason we chose not to use nix, however, pertains to how we model
dependencies and thus configuration. Consider the following workflow, rendered
lovingly in bad ASCII art:

```
             / baz \
foo --> bar <       > quz --> end
             \ qax /
```

In nix, one would represent this by making the `end` step take `quz` as an
input. `quz` would then depend on `baz` and `qax`, and so on down the chain.
When I asked for the output of `end`, by calling `nix-build -A end`, nix would
evaluate the tree and work out what it needed to build.

The problem with this model is that, if we have configuration parameters to
`bar`, say, they must be represented in `end`, to allow them to be passed down
through the chain. If we want to switch out `foo` for another tool `foo++`, we
need to modify all the subsequent steps. This is usually not a problem for
package managers (although it can be an issue), but in our case the likelihood
of wanting to modify the early steps of a pipeline is high.

In funflow, what we try to build is not the final output, `end`, but the entire
workflow. Each step knows nothing about the other steps, because this
information is encoded in the workflow definition. So modifying individual steps
is very simple, and often doesn't require any changes in the other steps.

These approaches are dual to each other, and it's certainly possible to
translate between them. But taking this approach definitely makes things easier
for our use cases.

## Conclusion

We think funflow is a nice addition to the space of workflow management tools,
and we're looking forward to applying it in more cases. So far we've used it to
manage the build and deployment of a project at one client, and at another for
data visualisation and for running a bioinformatics pipeline. We're also
gradually adding various bells and whistles to make it easier to use - this
summer we hope to have an intern working on adding graphical composition of
workflows so one can simply drag and drop them into place.

If you have a problem you'd like to apply funflow to, then please [check out the
repo](https://github.com/tweag/funflow) or get in touch.

