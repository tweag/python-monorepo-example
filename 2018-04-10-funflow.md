---
title: "Funflow"
shortTitle: "Funflow"
author: "Nicholas Clarke"
---

In this blog post, we'd like to officially announce Funflow, a system we've been
working on for the last few months with a small number of clients. Funflow is a
system for building and running workflows. In this blog post we'll talk about
what that entails, why we built funflow, and what we'd like to do with it in the future.

What is a workflow?
========

At its core, a workflow takes some inputs and produce some outputs, possibly
producing some side effects along the way. Of course, this description basically
describes any program. Workflow systems distinguish themselves in a few ways:

- A workflow is often composed of a number of steps, which may themselves be
  independent programs (or workflows). Composition of this higher level of
  programs might be done by domain specialists rather than programmers.
- Workflows are often long running, with some likelihood of failure halfway
  through. In such cases, we would want to resume them midway through without
  needing to rerun the earlier stages.
- Likewise, workflows may often need to be run on clusters or across distributed
  systems.

Workflow systems have perhaps seen the most use in the business process space,
where they allow non-programmers to automate business processes (such as
authorising a purchase order) which may involve multiple steps, some of which
are automatic, and others which might involve human intervention.

Another area heavily involving workflows, and one in which we are particularly
interested, is in scientific data processing. For example, in bioinformatics
one might develop a workflow (also often called 'pipeline' in this space) to
perform some in-silico analysis of sequenced gene data. Such a workflow will
be customised for a particular task, and may then be run for each sequenced
sample.

Wherefore funflow?
=====

There are innumerable workflow systems already out there, with myriad different
features. Tools like [Apache Taverna](https://taverna.incubator.apache.org/)
provide a very mature solution for building enterprise workflows, with
integrations for working with systems such as Hadoop and powerful GUI editors
for composing workflows. At the other end of the scale,
[luigi](http://luigi.readthedocs.io) and [airflow](https://airflow.apache.org/)
are libraries for composing workflows in Python code.

So why build a new one? It mostly came down to two things: integration and
caching. We'll spend the majority of this post discussing caching, so let's talk
about integration first.

Most of the simple workflow systems we looked at were designed to be run from a
command line, by a person or at best by a tool like `cron` or `at`. We find,
however, that we often want to run a workflow as part of a larger application.
For example, when doing data analysis one might wish to run classifiers on
different subsets of data, and then visualise the results. We still want to be
able to use classifiers written in any language or running on a different
machine, but we should be able to track the progress in a surrounding program,
and easily get the results back. When we tried to do this with Luigi, we found
ourselves needing to parse the log output just to find out the ID of the job
we'd triggered! So integration, for us, means:

- First class support for pulling values into, and out of, the host program.
- Ability to trigger a pipeline from within your program and deal with its
  output as any other value.

Funflow's workflows are just Haskell programs, built using
[arrow](https://www.haskell.org/arrows/) syntax. With funflow, we can easily
intermix steps done _inside_ our Haskell process and steps done _outside_, for
example by another program. Arrows give us the generality to model multiple
types of computation - a function `a -> b` is an arrow, as is a monadic function
`Monad m => a -> m b`, or a stream transformer `Stream a -> Stream b`. But
because they're less powerful than monads, we also get the ability to introspect
them and do things like drawing the dependency graph.

Caching
---

> There are two hard problems in programming: naming things, cache invalidation,
> and off by one errors. 

Consider the following two workflows, expressed as Haskell functions:

```haskell
flow1 :: Foo -> Bar
flow1 = extractBar . someExpensiveComputation . someTransformation

flow2 :: Foo -> Baz
flow2 = extractBaz . someExpensiveComputation . someTransformation
```

What we notice about these is that the first part of the computation is shared
between `flow1` and `flow2`. If we feed the same input `Foo`, we would
particularly like not to repeat `someExpensiveComputation`.

This is not an idle example; we often see workflows where part of the workflow
involves preprocessing of a reference data set, which may be done multiple
times, either by different users or when running a pipeline multiple times.
Perhaps more importantly, it may often be desirable to tweak the parameters of
some late stage of processing and rerun the pipeline - again, without rerunning
the unchanged earlier parts.

In order to address this issue, funflow borrows a couple of ideas from the
[nix](https://nixos.org/nix/) package manager. The first of these is to remove
the notion that the user has any control over where and how the outputs of the
intermediate steps in workflows are stored. Instead of the user controlling
where files are output, funflow manages a section of the file system known as
the _store_. Entries inside the store are addressed by a unique hash (the second
idea borrowed from nix), determined by hashing both the inputs to a step and the
definition of that step itself. When funflow executes a step in a workflow, it
first determines the hash of the inputs and the step definition to determine the
output path. If this path already exists (since store items are immutable once
written), we can skip the computation and use the result from the cache.

Funflow goes further than nix does, however. Whilst the hash of the inputs and
step definition determines the path to which the step writes its output, upon
completion of the step these outputs are moved into another path determined by
their _own_ hash: in other words, the store also works as [content addressable
storage](https://en.wikipedia.org/wiki/Content-addressable_storage). What's the
benefit of this? Well, firstly, it ensures that when multiple steps produce the
same output, that output is cached only once on disk. However, it also solves
the problem suggested by the following flows:

```haskell

flow1 :: Int -> Bar
flow1 = extractBar . someExpensiveComputation . (* 2)

flow2 :: String -> Bar
flow2 = extractBar . someExpensiveComputation . length
```

In this example, the _tails_ of the flows are similar. If I provide `4` to the
first flow and `"workflow"` to the second, however, then these computations will
converge after their first steps, and before `someExpensiveComputation`. If a
nix derivation were used in this case, two outputs will be produced and
`someExpensiveComputation` run twice, because ultimately the inputs differ.
Funflow, on the other hand, allows computations to _converge_.

Doing sensible caching might seem like a poor reason to build our own workflow
system. In reality, though, it's a large part of what makes one up. Going back
to our concepts above on what characterises a workflow as different from any
other program, funflow's system for managing inputs and outputs addresses them
pretty directly:

- Since long running pipelines are likely to fail, it's important to make sure
  we can resume them from the point of failure, even if we've modified the
  pipeline in order to address that failure. At the same time, we need to be
  sure that we don't accidentally find ourselves reusing cached data if it's
  inappropriate.
- In order to ship computations between machines, we need to be sure that the
  full environment to run that computation is available on the target machine.
  By making this environment explicit, and constraining what can be in it,
  funflow can make this safe and easy to do.



