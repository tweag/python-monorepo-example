---
title: Hyperion - TODO
author: Nicolas Mattia
---

This is an introduction to [hyperion][hyperion], a Haskell benchmarking library
for the 21st century that focuses on data export rather than analysis. I'll
talk about the reasons that led Tweag I/O to working on [hyperion][hyperion]
and what problems it tries to solve, and then present some of hyperion
functionalities like uploading benchmark results to elastic search and plot
them on Kibana.

![nowhere](Data.ByteString.Lazy.last performance against string size (red) and number of chunks (green))

## Why

Sitting somewhere between coffee and unit tests on the list of every good
programmer's acquired tastes, benchmarks are an important tool in every
performance work. Before even starting to think about _how_ you are going to
optimize a particular program, it is important that you know _why_ (and _if_)
you need to gain performance, and _where_ there is performance to be gained.

>  We should forget about small efficiencies, say about 97% of the time:
>  premature optimization is the root of all evil.
>   - Donald Knuth

Measuring software performance through micro-benchmarking is pretty much a
solved problem --- at least in the Haskell space --- thanks to the amazing
[criterion][criterion] library. However there is still an open question about
how to present the benchmark results to the programmer. [criterion][], for
instance, presents some analysis (like mean duration and standard deviation).
[hyperion][] takes a different approach: it provides enough information for
dedicated tools to .... Such tools could perform regressions, plots, or
anything else you could dream of. Hyperion benchmarks your code and passes the
collected data further.


## Problems

However isolated benchmarks are rarely useful. You rarely care about exactly
how long it takes for a function to run. You want to be able to compare
benchmark results: before this commit, over time, etc. Which leads to other
problems like ensuring that all the benchmarks are run on the same hardware and
how to store those benchmarks. Some solutions have emerged for particular use
cases https://perf.rust-lang.org/
https://www.reddit.com/r/haskell/comments/5nczqt/eta_modern_haskell_on_the_jvm/
https://perf.haskell.org/ghc/

Those tools allow the benchmarks to be compared and visualized. But there are
are off-the-shelf tools that allow just that: Elastic Search, which allows you
to store, search and analyze your data, and Kibana, an Elastic Search plugin,
that allows you to visualize just about anything.

Let's see what this looks like in practice.

You'll need docker-compose, git.

``` shell
$ git clone git@github.com:deviantony/docker-elk.git
$ cd docker-elk
$ docker-compose up -d
$ curl localhost:9200
{
  "name" : "7pOsdaA",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "e_CQlDgCQ1qIHob-n9h3fA",
  "version" : {
    "number" : "5.5.2",
    "build_hash" : "b2f0c09",
    "build_date" : "2017-08-14T12:33:14.154Z",
    "build_snapshot" : false,
    "lucene_version" : "6.6.0"
  },
  "tagline" : "You Know, for Search"
}
```


### Create ES index


We'll create an ES index. The index is similar to a database. We will create a
JSON index specification. The index requires a "type" (here `all-benchmarks`),
which is similar to a table. We won't go into the details.
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html#create-index-settings

``` json
{
  "settings" : {
      "number_of_shards" : 1
  },
  "mappings" : {
    "all-benchmarks" : {
      "dynamic" : true,
      "properties" : {
        "bench_name": {
          "type": "keyword"
        },
        "time_in_nanos": {
          "type": "float"
        },
        "timestamp": {
          "type": "date",
          "format": "epoch_second"
        }
      }
    }
  }
}
```

`properties` specifies which fields we want to be present when a new ES
document is uploaded. In this case we'll have
    * `bench_name`: the benchmark name, as a `keyword`, which is a field type that ES can index
    * `time_in_nanos`: the measured execution time for the given benchmark
    * `timestamp`: an ES [date](todo.com) which will be used as the canonical timestamp (to clarify we specify the format, the number of seconds since the epoch).

`dynamic: true`: this tells ES to not freak out if other fields are present and
try to index them as well. Let's create the index:


``` shell
$ curl -X PUT 'http://localhost:9200/hyperion-test' --data @index.json
```

Let's create a dummy benchmark just to make sure that elastic search is
correctly setup:

``` shell
$ jq -cM -n --arg t "$(date +%s)" '{ bench_name: "test", time_in_nanos: 250, timestamp: $t }'
{"bench_name":"test","time_in_nanos":250,"timestamp":"1503996451"}
```

We can save that to a file (`bench_1.json`) and upload that to ES as a
document:

https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_automatic_id_generation

```
$ curl -X POST 'http://localhost:9200/hyperion-test/all-benchmarks' --data @bench_1.json
{"_index":"hyperion-test","_type":"all-benchmarks","_id":"AV4tLbX1lgimc-bF1ryM","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"created":true}%
```

We can list elastic search's documents and see that our dummy benchmark was
uploaded successfully:

``` shell
curl 'http://localhost:9200/hyperion-test/_search' | jq
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "hyperion-test",
        "_type": "all-benchmarks",
        "_id": "AV4tLbX1lgimc-bF1ryM",
        "_score": 1,
        "_source": {
          "bench_name": "test",
          "time_in_nanos": 250,
          "timestamp": "1503996489"
        }
      }
    ]
  }
}
```

Create some helpers to get the unix timestamp and commit hash from git and
write add them to hyperion's metadata:

``` shell
$ alias git_unix_timestamp="git show -s --format=%ct HEAD"
$ alias git_commit_hash="git rev-parse HEAD"
$ stack exec hyperion-micro-benchmark-example -- \
    --arg timestamp:"$(git_unix_timestamp)" \
    --arg commit:"$(git_commit_hash)"
```

This will produce the following JSON document (YMMV):


``` json
{
  "results": [
    {
      "cycles": null,
      "measurements": null,
      "garbage_collections": null,
      "time_in_nanos": 100.58560439560439,
      "bench_name": "n choose k:4:3",
      "alloc": null,
      "bench_params": [
        4,
        3
      ]
    }, ...
    ],
  "metadata": {
    "location": null,
    "timestamp": "2017-08-29T09:17:33.080757Z",
    "commit": "010ed2cc2129a8249786566dd224d2e059cfea13"
  }
}
```

we see that the metadata was correctly inserted.  We can then flatten and split
the benchmarks and upload them individually (in practice you'll probably want
to use ES' [bulk API](todo.com)):

``` shell
$ stack exec hyperion-micro-benchmark-example -- \
    --arg timestamp:"$(git_unix_timestamp)" \
    --arg commit:"$(git_commit_hash)" \
    | jq -cM '.results | .[]' \
    | xargs -I '{}' \
    curl -X POST 'http://localhost:9200/hyperion-test/all-benchmarks' \
        -H 'Content-Type: application/json'
        -d '{}'
```



### Kibana's Way


As mentioned earlier, we also spinned up a Kibana container, accessible at
`localhost:5601`. However if try to access the page now you'll most likely get
an error, since we haven't configured Kibana yet. We need to tell it what ES
indices it should gather the data from. Let's tell Kibana to use any index
that's prefixed with `hyperion` (it's a bit confusing that we upload Kibana's
confing through ES). We'll first create the index then tell Kibana to use that
index as the default index:

https://www.elastic.co/guide/en/kibana/current/index-patterns.html

``` shell
$ curl -X PUT 'http://localhost:9200/.kibana/index-pattern/hyperion*' -H 'Content-Type: application/json' -d '{"title": "all hyperion", "timeFieldName": "timestamp"}'
{"_index":".kibana","_type":"index-pattern","_id":"hyperion*","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"created":true}%
$ curl -X PUT 'http://localhost:9200/.kibana/config/5.5.2' -H 'Content-Type: application/json' -d '{"defaultIndex": "all hyperion"}'
{"_index":".kibana","_type":"config","_id":"5.5.2","_version":2,"result":"updated","_shards":{"total":2,"successful":1,"failed":0},"created":false}%
```

You should now be able to access Kibana by opening your browser at the address
http://localhost:5601


## Hyperion

[hyperion][hyperion] is not a replacement for [criterion][criterion], rather it
is a place for trying out new ideas. We wanted a lab where we could develop and
experiment with new features, which might in turn be contributed to
[criterion][criterion] itself.

* same API, compatible criterion


[hyperion]: https://github.com/tweag/hyperion
[criterion]: https://github.com/bos/criterion
