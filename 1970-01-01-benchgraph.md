---
title: "Benchmarks History With Rshiny and jq"
author: "Théophane Hufschmitt"
---

If you're a conscientious developer like ~~I am~~ my boss is, you probably have
a benchmark suite for the programs and libraries you develop. This allows you
to see the impact your changes have on the performances of your application.

That's the situation I was in recently on a project I'm working on.
We had many carefully crafted benchmarks for all the performance-sensitive
parts of our code which we ran daily on our CI and were exported as a nice html
page such as [this one][criterion-html-sample].
But to be honest, hardly anyone looked at them, for the simple reason that
looking at a given benchmark result was most of the time absolutely meaningless.
The only sensible thing to do was to compare the results, but err… that
basically meant putting the two web pages side to side on one's screen and
manually comparing them value by value to see what changed, which was
incredibly painful and error-prone (and I speak from experience, having to do it
more than once).

[criterion-html-sample]: http://www.serpentine.com/criterion/report.html

Obviously, unless we had a *really* compelling reason to do so (like the users
calling out for help because their program was suddently running twice slower
than before), we just didn't look at them. And obviously this means that we've
accidentally introduced several annoying performance regressions without even
noticing it.

So we decided to give ourselves a way to quickly view the evolution of the
performances of our library through time, which meant:

- Keeping a record of our benchmark results
- Providing a way to display and analyze them

## Setting-up a history of our benchmarks

[hyperion-post]: https://www.tweag.io/posts/2017-09-06-hyperion.html

It happens that someone has already written here [on that topic][hyperion-post],
but the procedure presented here had two drawbacks:

- It required the use of a particular and experimental benchmark framework
  (`hyperion`), while we were using the much more mainstream `criterion`
- The results were stored and analyzed using `elasticsearch` and `kibana`,
  which while extremely powerful and flexible is also known to be a huge pain
  to maintain.
  And given the low volume of data we're gonna manipulate (each
  benchmark run produces slightly less than 20K of data, so even with a few
  hundred of benchmarks we're still in a range that even the smallest AWS
  instances can manage) there's no need for such a beast.

So while retaining the same basic idea, we've decided to adapt this approach
for something simpler to set up, i.e.

- An [r-shiny][r-shiny]-based visualisation tool taking as input a stream of
  json records containing all our benchmarks results.
- A simple script using [jq][jq] to format the output of `criterion` to the format
  expected by the R script

[r-shiny]: https://shiny.rstudio.com/
[jq]: https://stedolan.github.io/jq/

Note that these two parts are largely independent: The json emitted by the `jq`
script is very close to the one emitted by `hyperion` and could easily be fed
to any tool (such as the elasticsearch cluster like proposed in the [hyperion
post][hyperion-post]), and conversely, the webapp accepts a really simple
format which doesn't have to be generated by criterion.

Note also that this won't get all the benefits that `hyperion` brings over
`criterion`, in particular we don't get the possibility to show the duration of
an operation as a function of its input size.

### Simplifying Criterion's output

`Criterion` is able to export the results of the benchmarks in a `json` file,
which allows us to analyze them further.
We need however to trim that output to make it easy to import on the web
application: `criterion` only dumps its big internal state in it, including all
the runs it does for each benchmark (because each bench is run a few hundred
times to limit the inevitable noise) and a lot of analyzes on them which we
don't need either.
It also doesn't include some useful information for us, such as the rev
and the date of the commit this benchmark ran on (to identify it later).

Thanks to the wonders of `jq`, it is only a matter of a few lines of code to
transform [this](/criterion-output.json) into:

```json
{
  "time_in_nanos":0.005481833334197205,
  "bench_name":"1000 RandomTrees/Evaluation/Safe Val",
  "commit_rev":"dd4e6c36b913006d768d539bfc736bf574043e20",
  "timestamp":1535981516
}
{
  "time_in_nanos":0.004525946121560489,
  "bench_name":"1000 RandomTrees/Evaluation/Safe Val Int Tree",
  "commit_rev":"dd4e6c36b913006d768d539bfc736bf574043e20",
  "timestamp":1535981516
}
```

### Visualisation interface

*Disclaimer: I'm definitely not an R expert, so if anything here makes cry the
R programmer in you, don't worry, that's totally normal and expected*

We now want to build a nice graph UI for our benchmarks.
Let's first try to make our goals a bit more clear. We want:

- A chart displaying the metrics for the benchmarks through time
- A way to select which benchmarks to display

Thanks to the rich R ecosystem, this is easy to achieve. In addition to
`r-shiny` for the server part, we leverage [ggplot][ggplot] and
[plotly][plotly] for the graph, as well as the `pickerInput` from
[shinyWidgets][shinyWidgets] for the benchmark selection. This allows us to
quickly build a nice interactive graph to compare all our commits.

<img title="Sample benchmark graph" alt="Sample benchmark graph" src="../img/posts/benchgraph_screenshot.png" style="max-width: 100%; max-height: 100%"></img>

[ggplot]: https://ggplot2.tidyverse.org/
[plotly]: https://plot.ly/r/
[shinyWidgets]: https://rdrr.io/cran/shinyWidgets/

## Deploying the service

For this to be useful, there are two things we should do:

1. Running the benchmarks on a regular basis,
2. Uploading the results to a well-known place.

The CI is the natural place for that. Note however that if you're using a
hosted CI you're condamned to an inevitable noise since you'll have to run your
benchmarks on shared machines with unpredictible performances.
This can generally be partially mitigated by using a proper VM instead of a
docker container.

Let's see how this would look like on [Circle CI][circleci]:

[circleci]: http://circleci.com/

We can configure our benchmark job as follows:

```yaml
version: 2

jobs:
  build:
    # Use a VM instead of a docker container for more predictible performances
    machine: true
    steps:
      - checkout
      - run:
        name: Build
        command: |
          # Or `stack build :my-benchmark` or anything else
          bazel build //my:benchmark
      - run:
        name: Benchmark
        command: |
          # Run the benchmark suite
          bazel run //my:benchmark -- --json=raw_benchs.json

          # Install benchgraph and its dependency jq
          git clone https://github.com/novadiscovery/benchgraph
          apk install jq

          # Reformat the criterion output
          bash benchgraph/adapters/criterion/export_benchs.sh raw_benchs.json \
            > benchs.json

          # Send the output to s3.
          # This requires that your AWS credentials are set in CircleCI's config
          aws s3 cp benchs.json s3://my-benchmarks-output-bucket/${CIRCLE_SHA1}.json
```

With that, we can now generate the graph (locally for the sake of the
demonstration):

```sh
mkdir benchmarks
aws s3 cp --recursive s3://my-benchmarks-output-bucket/ benchmarks/
docker pull novadiscovery/benchgraph
docker run \
  -p 8123:8123
  -v $PWD/benchmarks:/benchmarks novadiscovery/benchmarks \
  /bin/benchgraph /benchmarks
```

The resulting graph is now available at <http://localhost:8123>

## Going further: multi-language benchmarks

It appears (or at least, so I heard) that the entire world isn't writing haskell
and that there are other languages out there, and different benchmark
frameworks. Does that mean reinventing all that for each language and each
framework? Of course not, although this has been developped in the context of
criterion, the only haskell-specific bit is the three-line long `jq` script
which converts criterion's output to a simple stream of json records.

So any benchmarking framework which provides a machine-readable output (which
hopefully means any benchmarking framework) can be easily adapted to this −
which also means that if you have a multi-language project, you can have all
your benchmarks integrated in a single interface for free.

Needless to say: PRs are welcome on [the github repo][benchgraph_github]

[benchgraph_github]: https://github.com/novadiscovery/benchgraph
