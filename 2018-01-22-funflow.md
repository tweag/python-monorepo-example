---
title: "Funflow: functional workflows"
shortTitle: "Funflow"
author: "Nicholas Clarke"
---

In data analysis, scientific computing or many other fields, we often find
ourselves doing the following:

- Take some input (perhaps deriving from an experiment or upstream process).
- Apply some particular processing steps.
- Examine the output.

Of course, this template can be applied to just about all problems. Where the
processing is highly specialised, we might write a new program to handle it.
Where the processing is very generic, we might find a single existing tool which
will do the job. Often, however, we find ourselves in the space where we wish to
apply a succession of existing tools to the problem, perhaps with some data
wrangling in between. It's this space which is the province of workflow systems.

What is a workflow system?
=======

At its simplest, a workflow system is a way of composing programs (often
pre-existing) together to create a single program. The prototypical example is
the humble unix pipe:

```bash
find . -maxdepth 2 -type f | xargs wc | sort -n
```

Of course, this model is also very familiar to us as functional programmers:

```haskell
words >>> map length >>> filter (\x -> x `mod` 3)
  $ "The quick brown fox jumped over the lazy dog"
```

A workflow system is an implementation of this idiom, typically adding a
number of other features appropriate to larger scale workflows composed of
steps which might not be under the user's control. These include things
such as:

- Handling failures in the middle of the pipeline without losing all the
  work done up to the point of that failure.
- Distributing work across multiple machines.
- Graphical composition and inspection of workflows.
- Multiple inputs and outputs from steps.

Workflow systems have perhaps seen the most use in the business process space,
where they allow non-programmers to automate business processes (such as
authorising a purchase order) which may involve multiple steps, some of which
are automatic, and others which might involve human intervention.

Another area heavily involving workflows, and one in which we are particularly
interested, is in scientific data processing. For example, in bioinformatics
one might develop a workflow (also often called 'pipeline' in this space) to
perform some in-silico analysis of sequenced gene data. Such a workflow will
be customised for a particular task, and may then be run for each sequenced
sample.

Wherefore funflow?
=====

There are a plethora of workflow systems available in the open source world.
Tools like [Apache Taverna](https://taverna.incubator.apache.org/) provide a
very mature solution for building enterprise workflows, with integrations for
working with systems such as Hadoop and powerful GUI editors for composing
workflows. At the other end of the scale, [luigi](http://luigi.readthedocs.io)
and [airflow](https://airflow.apache.org/) are libraries for composing workflows
in Python code. So what's the justification for a new one?

In our experience with existing workflow systems, we found a number of prevalent
issues:

- Whilst the 'enterprise' workflow systems have a lot of features, they also
  have a lot of boilerplate; service descriptions in WSDL, heavy configuration etc.
- The systems we tested were not designed to integrate well inside other
  programs. We wanted to trigger a workflow as part of the execution of another
  system for visualising results, but found ourselves lacking an API other than
  parsing results on the command line.
- Across the board, we were very dissatisfied with the handling of shared
  dependencies; e.g. results which are shared between multiple downstream tasks.
  Whilst it was generally possible to induce these, it required explicit
  declaration of such, and no consideration was given to whether results had
  perhaps changed.

Funflow is a workflow system for Haskell based on
[arrows](https://www.haskell.org/arrows/). Arrow can be seen as a generalisation
of functions. A function `a -> b` is an arrow, as is a monadic function `Monad m
=> a -> m b`, or a stream transformer `Stream a -> Stream b`. This makes arrows
a suitable model for workflows, where we wish to encapsulate each of these types
of computations. Funflow has a number of features unique (to the best of our
knowledge) amongst workflow systems, which we shall now explore.

Caching and provenance
-----

Consider the following two workflows, expressed as Haskell functions:

```haskell
flow1 :: Foo -> Bar
flow1 = extractBar . someExpensiveComputation . someTransformation

flow2 :: Foo -> Baz
flow2 = extractBaz . someExpensiveComputation . someTransformation
```

What we notice about these is that the first part of the computation is shared
between `flow1` and `flow2`. If we feed the same input `Bar`, we would
particularly like not to repeat `someExpensiveComputation`.

This is not an idle example; we often see workflows where part of the workflow
involves preprocessing of a reference data set, which may be done multiple
times, either by different users or when running a pipeline multiple times.
Perhaps more importantly, it may often be desirable to tweak the parameters of
some late stage of processing and rerun the pipeline - again, without rerunning
the unchanged earlier parts.

In order to address this issue, funflow borrows a couple of ideas from the
[nix](https://nixos.org/nix/) package manager. The first of these is to remove
the notion that the user has any control over where and how the outputs of the
intermediate steps in workflows are stored. Instead of the user controlling
where files are output, funflow manages a section of the file system known as
the _store_. Entries inside the store are addressed by a unique hash (the second
idea borrowed from nix), determined by hashing both the inputs to a step and the
definition of that step itself. When funflow executes a step in a workflow, it
first determines the hash of the inputs and the step definition to determine the
output path. If this path already exists (since store items are immutable once
written), we can skip the computation and use the result from the cache.

Funflow goes further than nix does, however. Whilst the hash of the inputs and
step definition determines the path to which the step writes its output, upon
completion of the step these outputs are moved into another path determined by
their _own_ hash: in other words, the store also works as [content addressable
storage](https://en.wikipedia.org/wiki/Content-addressable_storage). What's the
benefit of this? Well, firstly, it ensures that when multiple steps produce the
same output, that output is cached only once on disk. However, it also solves
the problem suggested by the following flows:

```haskell

flow1 :: Int -> Bar
flow1 = extractBar . someExpensiveComputation . (* 2)

flow2 :: String -> Bar
flow2 = extractBar . someExpensiveComputation . length
```

In this example, the _tails_ of the flows are similar. If I provide `4` to the
first flow and `"workflow"` to the second, however, then these computations will
converge after their first steps, and before `someExpensiveComputation`. If a
nix derivation were used in this case, two outputs will be produced and
`someExpensiveComputation` run twice, because ultimately the inputs differ.
Funflow, on the other hand, allows computations to _converge_.

Whilst caching is our primary motivator for taking this approach, it's very much
not the only benefit of the store. It makes it much easier to package everything
required for an experiment to send to another user, for example. Another option 
is to track the resources required to generate a result. In shared environments 
with resource constraints, these data can be used to perform intelligent garbage
collection on the store. This would remove items which were infrequently accessed
or could be easily (cheaply) reproduced.

Internal and external computation 
-----

Typically workflow systems work in one of two ways:

- 'Internal' systems. In this case, the workflow is manipulating data inside the
  host language's platform. For example, most of the examples in this post so
  far have been 'internal' flows: they are transforming Haskell types. This
  has the benefit of using the host's type system to ensure that flows are
  sensible. However, it constrains the set of tools one can use in the workflow. 
- 'External' systems. In this case, the workflow orchestrates tools outside the
  host language. Inputs to the system consist of arguments to command-line calls,
  and the actual computation is largely opaque to the workflow system. This allows
  the workflow to take advantage of the plethora of tools available, but makes it
  harder to integrate with a host application, and renders mistakes very easy to
  make.
  
Funflow support both of these paradigms, and clearly distinguishes them. A
workflow can consist of _internal_ tasks, which might be pure or effectful
functions operating on standard Haskell types, and _external_ tasks, which can
be any program which can be invoked on the command line, and operate on items in
the store. Funflow supports transitioning between these perspectives through
customisable serialisation.

For external tasks, Funflow has specific support for using docker to isolate
processing. Since docker containers can be identified by hash, this allows the
same guarantees on reproducibility as applies to content in the store. It also
makes it very easy to take advantage of the wise selection of tools packaged
as docker containers.

Here's an example of a workflow for compiling C sources, using both internal
and external steps, and using docker containers:

```Haskell
-- | This flow takes a string which is assumed to be the source code
--   for a 'C' function. It writes this to a file, then uses two external
--   steps to compile and run the function. The resulting 'stdout' is read
--   in and presented to the user.
compileAndRunC :: SimpleFlow String String
compileAndRunC = proc csrc -> do
    cInput <- writeString -< (csrc, [relfile|out.c|])
    scriptInput <- writeExecutableString -< (compileScript, [relfile|compile.sh|])
    compiled <- compileDocker -< (cInput, scriptInput)
    result <- runDocker -< compiled
    readString_ -< result
  where
    compileScript =
      " #!/usr/bin/env nix-shell \n\
      \ #! nix-shell -i bash -p gcc \n\
      \ gcc -o $2 $1 "

    compileDocker = docker $ \(cInput, scriptInput) -> Docker.Config
      { Docker.image = "nixos/nix"
      , Docker.optImageID = Just "1.11.14"
      , Docker.input = Docker.MultiInput
        $ Map.fromList [ ("script", CS.contentItem scriptInput)
                      , ("data", CS.contentItem cInput)
                      ]
      , Docker.command = "/input/script/compile.sh"
      , Docker.args = ["/input/data/out.c", "/output/out"]
      }
    runDocker = docker $ \input -> Docker.Config
      { Docker.image = "nixos/nix"
      , Docker.optImageID = Just "1.11.14"
      , Docker.input = Docker.SingleInput input
      , Docker.command = "bash -c"
      , Docker.args = ["\"/input/out > /output/out\""]
      }
```

Conclusion
=====

We developed Funflow for use integrating a machine learning pipeline into a data
visualisation system we were using, but we've already applied it in a couple of
other places as well: directly deployed as a bioinformatics pipeline and as a
build/deploy tool on another project.

Funflow is still a fairly small project, but we think it has a high power to
weight ratio. We have plans for a number of improvements, including adding
typing support to external steps through a _directory typing_ system and adding
graphical composition of workflows.

The code is available on [github](https://github.com/tweag/funflow), so please
take a look!
