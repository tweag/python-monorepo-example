---
title: "Funflow: functional workflows"
shortTitle: "Funflow"
author: "Nicholas Clarke"
---

In data analysis, scientific computing or many other fields, we often find
ourselves doing the following:

1. Take some input (perhaps deriving from an experiment or upstream process).
2. Apply some particular processing steps.
3. Examine the output.

Of course, this template can be applied to just about all problems. Where the
processing is highly specialised, we might write a new program to handle it.
Where it's very generic, we might find a single existing tool which will do the
job. Often, however, we find ourselves in the space where we wish to apply a
succession of existing tools to the problem, perhaps with some data wrangling in
between. It's this space which is the province of workflow systems.

What is a workflow system?
=======

At its simplest, a workflow system is a way of composing programs (often
pre-existing) together to create a single program. The prototypical example is
the humble unix pipe:

```bash
find . -maxdepth 2 -type f | xargs wc | sort -n
```

Of course, this model is also very familiar to us as functional programmers:

```haskell
let x = filter (\x -> x `mod` 3 == 0) . map length . words 
        $ "The quick brown fox jumped over the lazy dog"
```

A workflow system is an implementation of this idiom, typically adding a
number of other features appropriate to larger scale workflows composed of
steps which might not be under the user's control. These include things
such as:

- Handling failures in the middle of the pipeline without losing all the
  work done up to the point of that failure.
- Distributing work across multiple machines.
- Graphical composition and inspection of workflows.

Workflow systems have perhaps seen the most use in the business process space,
where they allow non-programmers to automate business processes (such as
authorising a purchase order) which may involve multiple steps, some of which
are automatic, and others which might involve human intervention.

Another area heavily involving workflows, and one in which we are particularly
interested, is in scientific data processing. For example, in bioinformatics
one might develop a workflow (also often called 'pipeline' in this space) to
perform some in-silico analysis of sequenced gene data. Such a workflow will
be customised for a particular task, and may then be run for each sequenced
sample.

There are a large number of workflow systems out there; some quite general,
others specialised to a particular application domain. In the next section we'll
discuss why none of them fit our needs, and the system we built to address
those needs.

Wherefore funflow?
=====

Funflow is a workflow system for Haskell based on
[arrows](https://www.haskell.org/arrows/). Arrow can be seen as a generalisation
of functions. A function `a -> b` is an arrow, as is a monadic function `Monad m
=> a -> m b`, or a stream transformer `Stream a -> Stream b`. This makes arrows
a suitable model for workflows, where we wish to encapsulate each of these types
of computations. Funflow has a number of features unique (to the best of our
knowledge) amongst workflow systems, which we shall now explore.

Consider the following two workflows, expressed as Haskell functions:

```haskell
flow1 :: Foo -> Bar
flow1 = extractBar . someExpensiveComputation . someTransformation

flow2 :: Foo -> Baz
flow2 = extractBaz . someExpensiveComputation . someTransformation
```

What we notice about these is that the first part of the computation is shared
between `flow1` and `flow2`. If we feed the same input `Bar`, we would
particularly like not to repeat `someExpensiveComputation`.

This is not an idle example; we often see workflows where part of the workflow
involves preprocessing of a reference data set, which may be done multiple
times, either by different users or when running a pipeline multiple times.
Perhaps more importantly, it may often be desirable to tweak the parameters of
some late stage of processing and rerun the pipeline - again, without rerunning
the unchanged earlier parts.

In order to address this issue, funflow borrows a couple of ideas from the
[nix](https://nixos.org/nix/) package manager. The first of these is to remove
the notion that the user has any control over where and how the outputs of the
intermediate steps in workflows are stored. Instead of the user controlling
where files are output, funflow manages a section of the file system known as
the _store_. Entries inside the store are addressed by a unique hash (the second
idea borrowed from nix), determined by hashing both the inputs to a step and the
definition of that step itself. When funflow executes a step in a workflow, it
first determines the hash of the inputs and the step definition to determine the
output path. If this path already exists (since store items are immutable once
written), we can skip the computation and use the result from the cache.

Funflow goes further than nix does, however. Whilst the hash of the inputs and
step definition determines the path to which the step writes its output, upon
completion of the step these outputs are moved into another path determined by
their _own_ hash: in other words, the store also works as [content addressable
storage](https://en.wikipedia.org/wiki/Content-addressable_storage). What's the
benefit of this? Well, firstly, it ensures that when multiple steps produce the
same output, that output is cached only once on disk. However, it also solves
the problem suggested by the following flows:

```haskell

flow1 :: Int -> Bar
flow1 = extractBar . someExpensiveComputation . (* 2)

flow2 :: String -> Bar
flow2 = extractBar . someExpensiveComputation . length
```

In this example, the _tails_ of the flows are similar. If I provide `4` to the
first flow and `"workflow"` to the second, however, then these computations will
converge after their first steps, and before `someExpensiveComputation`. If a
nix derivation were used in this case, two outputs will be produced and
`someExpensiveComputation` run twice, because ultimately the inputs differ.
Funflow, on the other hand, allows computations to _converge_. 
